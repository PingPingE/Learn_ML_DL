{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch15-2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMZiRK5PFMiFbbVrXsh076c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PingPingE/Learn_ML_DL/blob/main/Practice/Hands_On_ML/ch15-2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTfKFBs3C4TL"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow.keras as keras\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9pT0qxJy2tz"
      },
      "source": [
        "# 긴 시퀀스 다루기\n",
        "- 문제점 및 해결 방법(ch15-1에서 왜 tanh가 디폴트인지 알아보면서 살짝 다룸)\n",
        "  - gradient 소실 또는 폭주 문제 -> 불안정한 학습\n",
        "  - RNN이 긴 시퀀스를 처리할 때 입력의 첫 부분을 조금씩 잊어버리는 문제 \n",
        "\n",
        "- RNN에 잘맞는 종류의 정규화: <strong>층 정규화</strong>(ch13-2에서 살짝 다룸)\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcdaNCJ%2FbtqAa7RVwhD%2FP60H1iai2Mxgvu09TGQpv0%2Fimg.png\" width=80% height=80%/>\n",
        "\n",
        "  - 즉, 배치 정규화는 배치 차원에 대해 정규화, 층 정규화는 <strong>특성 차원에 대해 정규화</strong>\n",
        "  - 그래서 훈련과 테스트에서 동일한 방식으로 작동한다.\n",
        "    - 배치 정규화는 훈련 세트의 모든 세트에 대한 특성 통계를 추정하기 위해 지수 이동 평균이 필요했음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPJ3qhKy0e-C"
      },
      "source": [
        "## 메모리 셀 안에 층 정규화 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uw0lAKU0ejQ"
      },
      "source": [
        "class LNSimpleRNNCell(keras.layers.Layer):\n",
        "  def __init__(self, units, activation=\"tanh\", **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.state_size=units\n",
        "    self.output_size=units\n",
        "    self.simple_rnn_cell=keras.layers.SimpleRNNCell(units,activation=None)#===포인트: activation=None이다(활성화 함수에 넣기 전에 정규화하려고)\n",
        "    self.layer_norm=keras.layers.LayerNormalization()\n",
        "    self.activation=keras.activations.get(activation)\n",
        "  \n",
        "  def call(self, inputs, states):#====현재 타임 스텝의 inputs와 이전 타임 스텝의 hidden states(h_t-1)\n",
        "    outputs, new_states= self.simple_rnn_cell(inputs, states)\n",
        "    norm_outputs=self.activation(self.layer_norm(outputs)) #정규화하고 난 후의 값을 활성화 함수에 대입\n",
        "    return norm_outputs, [norm_outputs] #===두 개인 이유: 하나는 출력, 하나는 새로운 은닉 상태(h_t)가 된다\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDpPN0K_4FqX"
      },
      "source": [
        "- 사용자 정의 셀 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cowBiOFmzBpw"
      },
      "source": [
        "model=keras.models.Sequential([\n",
        "                               keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True, input_shape=[None, 1]),\n",
        "                               keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True),\n",
        "                               keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piylMGsX4d8F"
      },
      "source": [
        "-----------\n",
        "위 처럼 keras.layers.RNN층을 만들어서 LNSimpleRNNCell의 객체를 전달하면 된다.\n",
        "- 만약 타임 스텝 사이에 dropout을 적용하고 싶다면?\n",
        "  - 위처럼 드롭아웃을 적용하는 사용자 정의 셀을 만들어도 되긴하는데, \n",
        "  - keras.layers.SimpleRNN(recurrent_dropout=0.5) 매개변수를 지원함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX6N8thd5VQF"
      },
      "source": [
        "## LSTM 셀"
      ]
    }
  ]
}