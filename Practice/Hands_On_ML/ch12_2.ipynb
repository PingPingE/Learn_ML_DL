{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch12-2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNkquwRb9lg2mRT3Q2WdJZP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PingPingE/Learn_ML_DL/blob/main/Practice/Hands_On_ML/ch12_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY0BEQI8hHFM"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow import keras"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUUM2uvG7vcR"
      },
      "source": [
        "# 사용자 정의 모델\r\n",
        "- keras.Model 클래스를 상속\r\n",
        "- 생성자에서 층과 변수를 만든다\r\n",
        "- call()메서드에 모델이 해야 할 작업을 구현한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T10KFflo8Ew_"
      },
      "source": [
        "## 스킵 연결이 있는 사용자 정의 잔차 블록(ResidualBlock) 층을 가진 모델 만들어보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A70zKhiJiCkX"
      },
      "source": [
        "### Residual Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rciIssr8Hvtz"
      },
      "source": [
        "class ResidualBlock(keras.layers.Layer): #1. Layer클래스 상속\r\n",
        "  def __init__(self, n_layers, n_neurons, **kwargs): #2. 필요한 층을 만들고\r\n",
        "    super().__init__(**kwargs)\r\n",
        "    self.hidden=[keras.layers.Dense(n_neurons, activation='elu', kernel_initializer='he_normal') for _ in range(n_layers)] #n_layers개의 Dense layer\r\n",
        "\r\n",
        "  def call(self, inputs):#3. call메서드에 구현한다.\r\n",
        "    Z=inputs\r\n",
        "    for layer in self.hidden:\r\n",
        "      Z=layer(Z)\r\n",
        "    return inputs+Z #n_layers개의 은닉층을 거친 결과인 Z에 inputs값을 더해주기(skip connection)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCMnBd-miG1U"
      },
      "source": [
        "### Residual Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzMnCDanhu9n"
      },
      "source": [
        "class ResidualRegressor(keras.Model):#1. Model 클래스 상속\r\n",
        "  def __init__(self, output_dim, **kwargs): #2. 필요한 층을 만들고\r\n",
        "    super().__init__(**kwargs)\r\n",
        "    self.hidden1=keras.layers.Dense(30, activation='elu', kernel_initializer='he_normal')\r\n",
        "    self.block1=ResidualBlock(2,30) #30개의 유닛을 가진 hidden layer 2개 거친 후 skip connection\r\n",
        "    self.block2=ResidualBlock(2,30) #두 번째 잔차 블럭\r\n",
        "    self.out=keras.layers.Dense(output_dim)\r\n",
        "\r\n",
        "  def call(self,inputs): #3. call메서드에 구현\r\n",
        "    Z=self.hidden1(inputs)\r\n",
        "    for _ in range(3):#Residual block 여러 번 통과시키기\r\n",
        "      Z=self.block1(Z)\r\n",
        "    Z=self.block2(Z)\r\n",
        "    return self.out(Z)\r\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dnXr_kDlAUN"
      },
      "source": [
        "----------------\r\n",
        "- Model 클래스는 Layer 클래스의 서브클래스이므로 모델을 층처럼 정의할 수 있다.\r\n",
        "- 하지만 모델은 compile(), fit(), evaluate(), predict(), save() 등의 메서드와 같은 추가적인 기능이 있다.\r\n",
        "\r\n",
        "\r\n",
        "Q) Model이 더 많은 기능을 제공한다면, 모든 Layer를 Model처럼 정의하면 되지 않나?<br>\r\n",
        "A) 모든 층을 모델처럼 정의할 수 있지만, <strong>모델 안의 내부 구성 요소(재사용 가능한 층의 블럭 등)</strong>를 <strong>모델(훈련 대상 객체)과 구분</strong>하는 것이 당연하므로 Layer, Model 따로 상속해서 구현하는 것이다.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvESRafDmiLm"
      },
      "source": [
        "## 모델 구성 요소에 기반한 손실과 지표\r\n",
        "\r\n",
        "- 모델 구성 요소(은닉층의 가중치나 활성화 함수 등)에 기반한 손실을 정의하고 계산한다.\r\n",
        "- add_loss()메서드에 계산 결과를 전달한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q7m0vHxoqaW"
      },
      "source": [
        "### 사용자 정의 재구성 손실을 가지는 모델 만들기\r\n",
        "- 다섯 개의 은닉층과 출력층으로 구성된 회귀 MLP모델\r\n",
        "- 해당 모델은 맨 위의 은닉층에 보조 출력을 가진다. => 이 보조 출력에 연결된 손실이 재구성 손실\r\n",
        "- 재구성 손실: 보조 출력 값(재구성)과 input 값 사이의 오차(보통 오토인코더에 사용)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdDFa76hkibS"
      },
      "source": [
        "class ReconstructingRegressor(keras.Model):\r\n",
        "  def __init__(self, output_dim, **kwargs):\r\n",
        "    super().__init__(**kwargs)\r\n",
        "    self.hidden=[keras.layers.Dense(30, activation='selu', kernel_initializer='lecun_normal') for _ in range(5)]\r\n",
        "    self.out=keras.layers.Dense(output_dim)\r\n",
        "\r\n",
        "  def build(self, batch_input_shape): #Dense layer를 더 추가해서 모델의 입력을 재구성하는 데 사용한다.\r\n",
        "    n_inputs=batch_input_shape[-1] #유닛 개수는 입력 개수와 같아야 한다.\r\n",
        "    self.reconstruct=keras.layers.Dense(n_inputs)\r\n",
        "    super().build(batch_input_shape)\r\n",
        "\r\n",
        "  def call(self, inputs):\r\n",
        "    Z=inputs\r\n",
        "    for layer in self.hidden:\r\n",
        "      Z=layer(Z)\r\n",
        "    reconstruction=self.reconstruct(Z) #마지막 은닉층의 보조 출력 값\r\n",
        "    recon_loss=tf.reduce_mean(tf.square(reconstruction-inputs)) #재구성 손실: 보조 출력 값(재구성)과 input 값 사이의 평균 제곱 오차\r\n",
        "    self.add_loss(0.05*recon_loss) #모델의 손실 리스트에 추가한다.\r\n",
        "    return self.out(Z)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VqW6guRqbnu"
      },
      "source": [
        "------------\r\n",
        " 0.05(하이퍼파라미터)를 곱하는 이유는, 재구성 손실이 주 손실을 압도하지 않도록 하기 위해서다. (그냥 규제처럼 작용하기 위함)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5RgYZQXq_GF"
      },
      "source": [
        "## 자동 미분을 사용하여 Gradient 계산하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Zq95noFonpg"
      },
      "source": [
        "def f(w1,w2):\r\n",
        "  return 3*w1**2+2*w1*w2"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds7IahsjreRm"
      },
      "source": [
        "--------------\r\n",
        "- w1에 대한 함수 f의 도함수는 6*w1+2*w2\r\n",
        "- w2에 대한 함수 f의 도함수는 2*w1\r\n",
        "- (w1,w2) = (5, 3)에서 도함수의 값은 각각 36, 10 -> gradient vector: (36,10)\r\n",
        "\r\n",
        "-> 신경망은 보통 수만 개의 파라미터를 가지므로 이를 직접 다 계산하는 것은 불가능하다.<br>\r\n",
        "-> <strong>대안: 각 파라미터가 바뀔 때마다 함수의 출력이 얼마나 변하는지 측정해서 도함수의 근삿값을 계산하는 것</strong>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI9yAIZrrdtL",
        "outputId": "fc240737-062e-4ad0-9437-9372da58c6d7"
      },
      "source": [
        "w1,w2=5,3\r\n",
        "eps=1e-6\r\n",
        "(f(w1+eps,w2)-f(w1,w2))/eps"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36.000003007075065"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "453yPMjUsdda",
        "outputId": "16e7dda6-b2c9-4ecb-c9a5-ca3e0aeabfcd"
      },
      "source": [
        "(f(w1,w2+eps)-f(w1,w2))/eps"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.000000003174137"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kw2UXrizss4Y"
      },
      "source": [
        "------------\r\n",
        "하지만 파라미터마다 적어도 한번씩 f()를 호출한다. 따라서 대규모 신경망엔 적용하기 어렵다.<br>\r\n",
        "<strong>-> 대안: 자동 미분을 사용해보자</strong>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NCnHO0uzE2k"
      },
      "source": [
        "### GradientTape\r\n",
        "- 여러 값(보통 모델의 파라미터)에 대한 한 값(보통 손실값)의 gradient를 계산하는 데 사용\r\n",
        "- 한 번의 정방향 계산과 역방향 계산으로 모든 gradient를 동시에 계산할 수 있다.(후진 모드 자동 미분)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfB_swZrtGVB"
      },
      "source": [
        "w1,w2=tf.Variable(5.), tf.Variable(3.) #먼저 두 변수 w1,w2를 정의한다.\r\n",
        "with tf.GradientTape() as tape: # tf.GradientTape 블럭을 만들어서 두 변수와 관련된 모든 연산을 자동으로 기록한다.\r\n",
        "  z=f(w1,w2)\r\n",
        "gradients=tape.gradient(z,[w1,w2]) #tape에 두 변수 [w1,w2]에 대한 z의 gradient를 요청한다."
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8mqaD3ZtQ8T",
        "outputId": "6b8d2c2b-46ef-45af-a05c-25fe6f59ea42"
      },
      "source": [
        "gradients"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
              " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "rvzzwq1jwhDK",
        "outputId": "f9639740-2dec-43de-9986-9f237f0ae6b2"
      },
      "source": [
        "tape.gradients(z,[w1,w2]) #한 번 더 호출하면 error"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-e571b56dd2ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'GradientTape' object has no attribute 'gradients'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAsHocCTtSmA"
      },
      "source": [
        "--------------\r\n",
        "- gradient()메서드가 호출된 후에는 자동으로 테이프가 즉시 지워진다. -> 그래서 error 발생\r\n",
        "- 만약 한 번 이상 gradient()메서드를 호출해야한다면 persistent 값을 True로 주면 된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxhJVJw7w1yn",
        "outputId": "5e434d76-4999-46f5-9127-a7e56b7603a1"
      },
      "source": [
        "with tf.GradientTape(persistent=True) as tape: #persistent=True\r\n",
        "  z=f(w1,w2)\r\n",
        "gradients=tape.gradient(z,[w1,w2])\r\n",
        "tape.gradient(z,[w1,w2]) #한 번 더 호출해도 예외 발생 X"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
              " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KEMn_0Uwf9N"
      },
      "source": [
        "del tape #단, 테이프를 삭제해서 리소스를 해제해야한다. (tape 객체가 더 이상 유효하지 않게 되면 파이썬 GC가 삭제해준다.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCqZyOA2xznI"
      },
      "source": [
        "### Detail\r\n",
        "- 기본적으로 tape는 변수가 포함된 연산만을 기록한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zApRDYeByrnu"
      },
      "source": [
        "#### 만약 변수가 아니라면?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x05ANh3zyGve",
        "outputId": "f35b723b-f834-472d-c946-ebad6b656ae5"
      },
      "source": [
        "c1,c2=tf.constant(5.), tf.constant(3.) #상수\r\n",
        "with tf.GradientTape() as tape:\r\n",
        "  z=f(c1,c2)\r\n",
        "tape.gradient(z,[c1,c2])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2lv8Wy0ySRy"
      },
      "source": [
        "-------------\r\n",
        "계산이 안된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BX2EzAwyQcP",
        "outputId": "f4774ceb-1ad1-41ec-95d4-42bbca1c2273"
      },
      "source": [
        "with tf.GradientTape() as tape:\r\n",
        "  tape.watch(c1) \r\n",
        "  tape.watch(c2)\r\n",
        "  z=f(c1,c2)\r\n",
        "tape.gradient(z,[c1,c2])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
              " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4Slllomydpd"
      },
      "source": [
        "---------------\r\n",
        "하지만 watch()메서드로 어떤 텐서라도 감시해서 모든 연산을 기록할 수 있도록 강제할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el8YhHSDzpa0"
      },
      "source": [
        "#### 신경망의 일부분에 gradient가 역전파되지 않도록 막고 싶다면?\r\n",
        "- stop_gradient(): 역전파 시에 상수처럼 동작한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nxy29eTQzttw",
        "outputId": "0da71e10-6baa-4ecd-82df-3535457e3245"
      },
      "source": [
        "def f(w1,w2):\r\n",
        "  return 3*w1**2+tf.stop_gradient(2*w1*w2) #역전파 시, 2*w1*w2를 상수 취급한다.\r\n",
        "\r\n",
        "with tf.GradientTape() as tape:\r\n",
        "  z=f(w1,w2)\r\n",
        "\r\n",
        "tape.gradient(z,[w1,w2])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(), dtype=float32, numpy=30.0>, None]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lbwzi5cr0kLW"
      },
      "source": [
        "#### 수치적인 이슈\r\n",
        "- 큰 입력에 대한 my_softplus()함수의 gradient를 계산하면 NaN이 반환된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y--nFAj2055w"
      },
      "source": [
        "def my_softplus(z):\r\n",
        "  return tf.math.log(tf.exp(z)+1.0)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXYhgLQc3FCB",
        "outputId": "ebfc7dca-ccb6-41be-de5c-0d12e17e0c78"
      },
      "source": [
        "tf.exp(100.) #tf.exp는 입력값이 크면 np.inf를 반환한다."
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=inf>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpTcArg7xx3O",
        "outputId": "a7e185b2-fef7-41f3-e67a-fca1bf2f73ce"
      },
      "source": [
        "x=tf.Variable([100.])\r\n",
        "with tf.GradientTape() as tape:\r\n",
        "  z=my_softplus(x)\r\n",
        "tape.gradient(z,[x])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(1,), dtype=float32, numpy=array([nan], dtype=float32)>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYJhNrTO0-t6"
      },
      "source": [
        "---------\r\n",
        "- 부동소수점 정밀도 오류로 인해서 자동 미분이 무한 나누기 무한을 하게 된다. -> 그래서 NaN 반환"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOAtAZIu2eK0"
      },
      "source": [
        "@tf.custom_gradient 데코레이터로 수치적으로 안전한 softplus의 도함수를 계산해서 반환하는 함수 작성하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Egm54jFB1Mah"
      },
      "source": [
        "@tf.custom_gradient\r\n",
        "def my_better_softplus(z):\r\n",
        "  exp=tf.exp(z)\r\n",
        "\r\n",
        "  def my_softplus_gradients(grad):\r\n",
        "    return grad/(1+1/exp) #exp가 np.inf면 -> grad/(1+0)이 된다\r\n",
        "  \r\n",
        "  return tf.math.log(exp+1), my_softplus_gradients()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbJVOuXfsqLw"
      },
      "source": [
        ""
      ]
    }
  ]
}