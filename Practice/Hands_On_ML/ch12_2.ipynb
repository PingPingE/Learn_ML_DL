{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch12-2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNbsMVQaZQyc8gF+SrC/GGO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PingPingE/Learn_ML_DL/blob/main/Practice/Hands_On_ML/ch12_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY0BEQI8hHFM"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUUM2uvG7vcR"
      },
      "source": [
        "# 사용자 정의 모델\r\n",
        "- keras.Model 클래스를 상속\r\n",
        "- 생성자에서 층과 변수를 만든다\r\n",
        "- call()메서드에 모델이 해야 할 작업을 구현한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T10KFflo8Ew_"
      },
      "source": [
        "## 스킵 연결이 있는 사용자 정의 잔차 블록(ResidualBlock) 층을 가진 모델 만들어보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A70zKhiJiCkX"
      },
      "source": [
        "### Residual Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rciIssr8Hvtz"
      },
      "source": [
        "class ResidualBlock(keras.layers.Layer): #1. Layer클래스 상속\r\n",
        "  def __init__(self, n_layers, n_neurons, **kwargs): #2. 필요한 층을 만들고\r\n",
        "    super().__init__(**kwargs)\r\n",
        "    self.hidden=[keras.layers.Dense(n_neurons, activation='elu', kernel_initializer='he_normal') for _ in range(n_layers)] #n_layers개의 Dense layer\r\n",
        "\r\n",
        "  def call(self, inputs):#3. call메서드에 구현한다.\r\n",
        "    Z=inputs\r\n",
        "    for layer in self.hidden:\r\n",
        "      Z=layer(Z)\r\n",
        "    return inputs+Z #n_layers개의 은닉층을 거친 결과인 Z에 inputs값을 더해주기(skip connection)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCMnBd-miG1U"
      },
      "source": [
        "### Residual Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzMnCDanhu9n"
      },
      "source": [
        "class ResidualRegressor(keras.Model):#1. Model 클래스 상속\r\n",
        "  def __init__(self, output_dim, **kwargs): #2. 필요한 층을 만들고\r\n",
        "    super().__init__(**kwargs)\r\n",
        "    self.hidden1=keras.layers.Dense(30, activation='elu', kernel_initializer='he_normal')\r\n",
        "    self.block1=ResidualBlock(2,30) #30개의 유닛을 가진 hidden layer 2개 거친 후 skip connection\r\n",
        "    self.block2=ResidualBlock(2,30) #두 번째 잔차 블럭\r\n",
        "    self.out=keras.layers.Dense(output_dim)\r\n",
        "\r\n",
        "  def call(self,inputs): #3. call메서드에 구현\r\n",
        "    Z=self.hidden1(inputs)\r\n",
        "    for _ in range(3):#Residual block 여러 번 통과시키기\r\n",
        "      Z=self.block1(Z)\r\n",
        "    Z=self.block2(Z)\r\n",
        "    return self.out(Z)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dnXr_kDlAUN"
      },
      "source": [
        "----------------\r\n",
        "- Model 클래스는 Layer 클래스의 서브클래스이므로 모델을 층처럼 정의할 수 있다.\r\n",
        "- 하지만 모델은 compile(), fit(), evaluate(), predict(), save() 등의 메서드와 같은 추가적인 기능이 있다.\r\n",
        "\r\n",
        "\r\n",
        "Q) Model이 더 많은 기능을 제공한다면, 모든 Layer를 Model처럼 정의하면 되지 않나?<br>\r\n",
        "A) 모든 층을 모델처럼 정의할 수 있지만, <strong>모델 안의 내부 구성 요소(재사용 가능한 층의 블럭 등)</strong>를 <strong>모델(훈련 대상 객체)과 구분</strong>하는 것이 당연하므로 Layer, Model 따로 상속해서 구현하는 것이다.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvESRafDmiLm"
      },
      "source": [
        "## 모델 구성 요소에 기반한 손실과 지표\r\n",
        "\r\n",
        "- 모델 구성 요소(은닉층의 가중치나 활성화 함수 등)에 기반한 손실을 정의하고 계산한다.\r\n",
        "- add_loss()메서드에 계산 결과를 전달한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q7m0vHxoqaW"
      },
      "source": [
        "### 사용자 정의 재구성 손실을 가지는 모델 만들기\r\n",
        "- 다섯 개의 은닉층과 출력층으로 구성된 회귀 MLP모델\r\n",
        "- 해당 모델은 맨 위의 은닉층에 보조 출력을 가진다. => 이 보조 출력에 연결된 손실이 재구성 손실\r\n",
        "- 재구성 손실: 보조 출력 값(재구성)과 input 값 사이의 오차(보통 오토인코더에 사용)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdDFa76hkibS"
      },
      "source": [
        "class ReconstructingRegressor(keras.Model):\r\n",
        "  def __init__(self, output_dim, **kwargs):\r\n",
        "    super().__init__(**kwargs)\r\n",
        "    self.hidden=[keras.layers.Dense(30, activation='selu', kernel_initializer='lecun_normal') for _ in range(5)]\r\n",
        "    self.out=keras.layers.Dense(output_dim)\r\n",
        "\r\n",
        "  def build(self, batch_input_shape): #Dense layer를 더 추가해서 모델의 입력을 재구성하는 데 사용한다.\r\n",
        "    n_inputs=batch_input_shape[-1] #유닛 개수는 입력 개수와 같아야 한다.\r\n",
        "    self.reconstruct=keras.layers.Dense(n_inputs)\r\n",
        "    super().build(batch_input_shape)\r\n",
        "\r\n",
        "  def call(self, inputs):\r\n",
        "    Z=inputs\r\n",
        "    for layer in self.hidden:\r\n",
        "      Z=layer(Z)\r\n",
        "    reconstruction=self.reconstruct(Z) #마지막 은닉층의 보조 출력 값\r\n",
        "    recon_loss=tf.reduce_mean(tf.square(reconstruction-inputs)) #재구성 손실: 보조 출력 값(재구성)과 input 값 사이의 평균 제곱 오차\r\n",
        "    self.add_loss(0.05*recon_loss) #모델의 손실 리스트에 추가한다.\r\n",
        "    return self.out(Z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VqW6guRqbnu"
      },
      "source": [
        "------------\r\n",
        " 0.05(하이퍼파라미터)를 곱하는 이유는, 재구성 손실이 주 손실을 압도하지 않도록 하기 위해서다. (그냥 규제처럼 작용하기 위함)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5RgYZQXq_GF"
      },
      "source": [
        "## 자동 미분을 사용하여 Gradient 계산하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Zq95noFonpg"
      },
      "source": [
        "def f(w1,w2):\r\n",
        "  return 3*w1**2+2*w1*w2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds7IahsjreRm"
      },
      "source": [
        "--------------\r\n",
        "- w1에 대한 함수 f의 도함수는 6*w1+2*w2\r\n",
        "- w2에 대한 함수 f의 도함수는 2*w1\r\n",
        "- (w1,w2) = (5, 3)에서 도함수의 값은 각각 36, 10 -> gradient vector: (36,10)\r\n",
        "\r\n",
        "-> 신경망은 보통 수만 개의 파라미터를 가지므로 이를 직접 다 계산하는 것은 불가능하다.<br>\r\n",
        "-> <strong>대안: 각 파라미터가 바뀔 때마다 함수의 출력이 얼마나 변하는지 측정해서 도함수의 근삿값을 계산하는 것</strong>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI9yAIZrrdtL",
        "outputId": "fc240737-062e-4ad0-9437-9372da58c6d7"
      },
      "source": [
        "w1,w2=5,3\r\n",
        "eps=1e-6\r\n",
        "(f(w1+eps,w2)-f(w1,w2))/eps"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36.000003007075065"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "453yPMjUsdda",
        "outputId": "16e7dda6-b2c9-4ecb-c9a5-ca3e0aeabfcd"
      },
      "source": [
        "(f(w1,w2+eps)-f(w1,w2))/eps"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.000000003174137"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kw2UXrizss4Y"
      },
      "source": [
        "------------\r\n",
        "하지만 파라미터마다 적어도 한번씩 f()를 호출한다. 따라서 대규모 신경망엔 적용하기 어렵다.<br>\r\n",
        "<strong>-> 대안: 자동 미분을 사용해보자</strong>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NCnHO0uzE2k"
      },
      "source": [
        "### GradientTape\r\n",
        "- 여러 값(보통 모델의 파라미터)에 대한 한 값(보통 손실값)의 gradient를 계산하는 데 사용\r\n",
        "- 한 번의 정방향 계산과 역방향 계산으로 모든 gradient를 동시에 계산할 수 있다.(후진 모드 자동 미분)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfB_swZrtGVB"
      },
      "source": [
        "w1,w2=tf.Variable(5.), tf.Variable(3.) #먼저 두 변수 w1,w2를 정의한다.\r\n",
        "with tf.GradientTape() as tape: # tf.GradientTape 블럭을 만들어서 두 변수와 관련된 모든 연산을 자동으로 기록한다.\r\n",
        "  z=f(w1,w2)\r\n",
        "gradients=tape.gradient(z,[w1,w2]) #tape에 두 변수 [w1,w2]에 대한 z의 gradient를 요청한다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8mqaD3ZtQ8T",
        "outputId": "6b8d2c2b-46ef-45af-a05c-25fe6f59ea42"
      },
      "source": [
        "gradients"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
              " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "rvzzwq1jwhDK",
        "outputId": "f9639740-2dec-43de-9986-9f237f0ae6b2"
      },
      "source": [
        "tape.gradients(z,[w1,w2]) #한 번 더 호출하면 error"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-e571b56dd2ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'GradientTape' object has no attribute 'gradients'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAsHocCTtSmA"
      },
      "source": [
        "--------------\r\n",
        "- gradient()메서드가 호출된 후에는 자동으로 테이프가 즉시 지워진다. -> 그래서 error 발생\r\n",
        "- 만약 한 번 이상 gradient()메서드를 호출해야한다면 persistent 값을 True로 주면 된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxhJVJw7w1yn",
        "outputId": "5e434d76-4999-46f5-9127-a7e56b7603a1"
      },
      "source": [
        "with tf.GradientTape(persistent=True) as tape: #persistent=True\r\n",
        "  z=f(w1,w2)\r\n",
        "gradients=tape.gradient(z,[w1,w2])\r\n",
        "tape.gradient(z,[w1,w2]) #한 번 더 호출해도 예외 발생 X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
              " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KEMn_0Uwf9N"
      },
      "source": [
        "del tape #단, 테이프를 삭제해서 리소스를 해제해야한다. (tape 객체가 더 이상 유효하지 않게 되면 파이썬 GC가 삭제해준다.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCqZyOA2xznI"
      },
      "source": [
        "### Detail\r\n",
        "- 기본적으로 tape는 변수가 포함된 연산만을 기록한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zApRDYeByrnu"
      },
      "source": [
        "#### 만약 변수가 아니라면?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x05ANh3zyGve",
        "outputId": "f35b723b-f834-472d-c946-ebad6b656ae5"
      },
      "source": [
        "c1,c2=tf.constant(5.), tf.constant(3.) #상수\r\n",
        "with tf.GradientTape() as tape:\r\n",
        "  z=f(c1,c2)\r\n",
        "tape.gradient(z,[c1,c2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2lv8Wy0ySRy"
      },
      "source": [
        "-------------\r\n",
        "계산이 안된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BX2EzAwyQcP",
        "outputId": "f4774ceb-1ad1-41ec-95d4-42bbca1c2273"
      },
      "source": [
        "with tf.GradientTape() as tape:\r\n",
        "  tape.watch(c1) \r\n",
        "  tape.watch(c2)\r\n",
        "  z=f(c1,c2)\r\n",
        "tape.gradient(z,[c1,c2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
              " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4Slllomydpd"
      },
      "source": [
        "---------------\r\n",
        "하지만 watch()메서드로 어떤 텐서라도 감시해서 모든 연산을 기록할 수 있도록 강제할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el8YhHSDzpa0"
      },
      "source": [
        "#### 신경망의 일부분에 gradient가 역전파되지 않도록 막고 싶다면?\r\n",
        "- stop_gradient(): 역전파 시에 상수처럼 동작한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nxy29eTQzttw",
        "outputId": "0da71e10-6baa-4ecd-82df-3535457e3245"
      },
      "source": [
        "def f(w1,w2):\r\n",
        "  return 3*w1**2+tf.stop_gradient(2*w1*w2) #역전파 시, 2*w1*w2를 상수 취급한다.\r\n",
        "\r\n",
        "with tf.GradientTape() as tape:\r\n",
        "  z=f(w1,w2)\r\n",
        "\r\n",
        "tape.gradient(z,[w1,w2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(), dtype=float32, numpy=30.0>, None]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lbwzi5cr0kLW"
      },
      "source": [
        "#### 수치적인 이슈\r\n",
        "- 큰 입력에 대한 my_softplus()함수의 gradient를 계산하면 NaN이 반환된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y--nFAj2055w"
      },
      "source": [
        "def my_softplus(z):\r\n",
        "  return tf.math.log(tf.exp(z)+1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXYhgLQc3FCB",
        "outputId": "ebfc7dca-ccb6-41be-de5c-0d12e17e0c78"
      },
      "source": [
        "tf.exp(100.) #tf.exp는 입력값이 크면 np.inf를 반환한다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=inf>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpTcArg7xx3O",
        "outputId": "a7e185b2-fef7-41f3-e67a-fca1bf2f73ce"
      },
      "source": [
        "x=tf.Variable([100.])\r\n",
        "with tf.GradientTape() as tape:\r\n",
        "  z=my_softplus(x)\r\n",
        "tape.gradient(z,[x])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(1,), dtype=float32, numpy=array([nan], dtype=float32)>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYJhNrTO0-t6"
      },
      "source": [
        "---------\r\n",
        "- 부동소수점 정밀도 오류로 인해서 자동 미분이 무한 나누기 무한을 하게 된다. -> 그래서 NaN 반환"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOAtAZIu2eK0"
      },
      "source": [
        "@tf.custom_gradient 데코레이터로 수치적으로 안전한 softplus의 도함수를 계산해서 반환하는 함수 작성하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Egm54jFB1Mah"
      },
      "source": [
        "@tf.custom_gradient\r\n",
        "def my_better_softplus(z):\r\n",
        "  exp=tf.exp(z)\r\n",
        "\r\n",
        "  def my_softplus_gradients(grad):\r\n",
        "    return grad/(1+1/exp) #exp가 np.inf면 -> grad/(1+0)이 된다\r\n",
        "  \r\n",
        "  return tf.math.log(exp+1), my_softplus_gradients()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbJVOuXfsqLw"
      },
      "source": [
        "## 사용자 정의 훈련 반복\r\n",
        "- fit()메서드의 유연성이 충분하지 않는 경우\r\n",
        "- 의도한대로 잘 동작하는지 확신을 갖고 싶은 경우\r\n",
        "- 위와 같은 경우 직접 훈련 반복을 만들 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7BQuv1M68Q7"
      },
      "source": [
        "### 간단한 모델 만들기\r\n",
        "- 훈련 반복을 직접 다루기에 컴파일할 필요가 없다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8oX9kl-6BIF"
      },
      "source": [
        "l2_reg=keras.regularizers.l2(0.05)\r\n",
        "model=keras.models.Sequential([\r\n",
        "                               keras.layers.Dense(30, activation='elu', kernel_initializer='he_normal', kernel_regularizer=l2_reg),\r\n",
        "                               keras.layers.Dense(1,kernel_regularizer=l2_reg)\r\n",
        "\r\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBfyw48c6Z6y"
      },
      "source": [
        "def random_batch(X,y,batch_size=32): #샘플 배치를 랜덤하게 추출하는 함수\r\n",
        "  idx=np.random.randint(len(X), size=batch_size)\r\n",
        "  return X[idx], y[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDK9_pP45dIA"
      },
      "source": [
        "def print_status_bar(iteration, total, loss, metrics=None): #훈련 상태 출력 함수\r\n",
        "  metrics=\" - \".join([f\"{m.name}:{m.result():.4f}\" for m in [loss]+(metrics or [])])\r\n",
        "  end=\"\" if iteration < total else \"\\n\"\r\n",
        "  print(f\"\\r{iteration}/{total}\" + metrics, end=end)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajBzsprf7mv7"
      },
      "source": [
        "### 적용해보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03ikCuEn9K3n"
      },
      "source": [
        "X_train=tf.constant([1,2,3,4,5])\r\n",
        "y_train=tf.constant([3,4,5,6,7])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q22oE9W97eph"
      },
      "source": [
        "n_epochs=5\r\n",
        "batch_size=32\r\n",
        "n_steps=len(X_train)//batch_size\r\n",
        "optimizer=keras.optimizers.Nadam(lr=0.01)\r\n",
        "loss_fn=keras.losses.mean_squared_error\r\n",
        "mean_loss=keras.metrics.Mean()\r\n",
        "metrics=[keras.metrics.MeanAbsoluteError()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3lpu5Wj71Y7",
        "outputId": "295aef0a-355b-4104-d1f8-93c96cd3d401"
      },
      "source": [
        "for epoch in range(1,n_epochs+1): #epoch\r\n",
        "  print(f\"epoch {epoch}/{n_epochs}\")\r\n",
        "  for step in range(1,n_steps+1): #batch\r\n",
        "    X_batch, y_batch = random_batch(X_train_scaled, y_train) #랜덤하게 샘플링\r\n",
        "    with tf.GradientTape() as tape: \r\n",
        "      y_pred =model(X_batch, training=True) #배치 하나를 위한 예측을 만들고\r\n",
        "      main_loss=tf.reduce_mean(loss_fn(y_batch, y_pred)) #손실을 계산하고 모든 샘플에 대한 평균 손실을 구한다\r\n",
        "      loss=tf.add_n([main_loss]+model.losses) #손실을 모두 더한다\r\n",
        "\r\n",
        "    gradients=tape.gradient(loss, model.trainable_variables) #훈련 가능한 변수에 대한 손실의 gradient 계산\r\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables)) #gradient descent 수행\r\n",
        "    mean_loss(loss)\r\n",
        "    for metric in metrics:\r\n",
        "      metric(y_batch, y_pred)\r\n",
        "    print_status_bar(step*batch_size, len(y_train), mean_loss, metrics)\r\n",
        "  print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\r\n",
        "  for metric in [mean_loss] + metrics:\r\n",
        "    metric.reset_states()\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1/5\n",
            "\r5/5mean:0.0000 - mean_absolute_error:0.0000\n",
            "epoch 2/5\n",
            "\r5/5mean:0.0000 - mean_absolute_error:0.0000\n",
            "epoch 3/5\n",
            "\r5/5mean:0.0000 - mean_absolute_error:0.0000\n",
            "epoch 4/5\n",
            "\r5/5mean:0.0000 - mean_absolute_error:0.0000\n",
            "epoch 5/5\n",
            "\r5/5mean:0.0000 - mean_absolute_error:0.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYqHadEw-wqp"
      },
      "source": [
        "---------\r\n",
        "- 이 처럼 사용자 정의 훈련 반복 구현은 주의해야 할 점이 많고, 실수하기 쉽다.\r\n",
        "- 장점은 완전히 제어할 수 있다는 것"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Hedow6o_IPL"
      },
      "source": [
        "## 텐서플로 함수와 그래프\r\n",
        "- 텐서플로 2에서는 1보다 훨씬 쉽게 그래프를 사용하기 쉽다.\r\n",
        "- 텐서플로 함수는 파이썬 함수보다 훨씬 빠르게 실행한다.(복잡한 연산을 수행할 때 더욱 두드러진다)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xt4vETIj_esc"
      },
      "source": [
        "### 일반 파이썬 함수 -> 텐서플로 함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4_wGDNJ_23C"
      },
      "source": [
        "#### 파이썬 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbkOxpFs--v4"
      },
      "source": [
        "def cube(x):\r\n",
        "  return x**3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJ-b85dy8uZf",
        "outputId": "518780e6-3ac5-4429-a819-177fadeb9710"
      },
      "source": [
        "cube(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEnYC9cf7S45",
        "outputId": "7ee08be6-35aa-48ac-ada2-2ec401a124f2"
      },
      "source": [
        "cube(tf.constant(2.0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-gBHUHD_5-H"
      },
      "source": [
        "#### 텐서플로 함수로 변환\r\n",
        "- 내부적으로 tf.function은 cube()함수에서 수행되는 계산을 분석하고 동일한 작업을 수행하는 계산 그래프를 생성한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gM2O3OW_Z7y",
        "outputId": "d5267fd6-0adf-4924-d442-65f48ae5ec0c"
      },
      "source": [
        "#방법 1: 함수 적용\r\n",
        "tf_cube=tf.function(cube)\r\n",
        "tf_cube "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.eager.def_function.Function at 0x7f12c61062d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3en1wGmWAing"
      },
      "source": [
        "#방법 2: 데코레이터\r\n",
        "@tf.function\r\n",
        "def tf_cube(x):\r\n",
        "  return x**3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG4K4sCL_d6B",
        "outputId": "2a26de13-f1b6-4a35-9340-c94040a89798"
      },
      "source": [
        "tf_cube(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int32, numpy=8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fuP-2HP_9iS",
        "outputId": "f48827a6-6162-4103-ad9b-33427d4739a3"
      },
      "source": [
        "tf_cube(tf.constant(2.0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnlJMfcZAAN-"
      },
      "source": [
        "-----------\r\n",
        "파이썬 함수 -> 텐서플로 함수로 변환 후, 텐서를 반환한다.\r\n",
        "- 주의: 파이썬 값으로 텐서플로 함수를 여러번 호출하면 프로그램이 느려진다.(매번 새로운 그래프를 생성해서)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2F-OW7NBA51"
      },
      "source": [
        "## 텐서플로가 그래프를 생성하는 방법은?\r\n",
        "<img src=\"https://pic1.zhimg.com/80/v2-be09ffe32f08eab81e78edec00f681b8_1440w.jpg\" width=70% height=70%/>\r\n",
        "\r\n",
        "1. 파이썬 함수의 소스 코드를 분석해서 for, while, if, break 등 제어문을 모두 찾는다. -> 이 단계를 \"오토그래프\"라고 한다.\r\n",
        "2. 1에서 찾은 제어문을 텐서플로 연산으로 바꾼 새로운 버전의 함수를 만든다.\r\n",
        "3. 텐서플로가 2에서 만들어진 함수를 호출한다. -> 해당 함수는 \"그래프 모드\"로 실행된다. \r\n",
        "4. 최종 그래프는 \"트레이싱\"과정을 통해 생성된다\r\n"
      ]
    }
  ]
}