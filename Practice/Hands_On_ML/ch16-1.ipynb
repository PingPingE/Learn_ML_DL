{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch16-1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1H8P_IzMWTl1xFbrXFPeJ8xz3Ut2edEUA",
      "authorship_tag": "ABX9TyMxVSwEiuQCyTUWjQ7ySOtD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PingPingE/Learn_ML_DL/blob/main/Practice/Hands_On_ML/ch16-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG3kbNEBQu4Z"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_gpu\n",
        "import tensorflow.keras as keras\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK8oyhuIJSS1"
      },
      "source": [
        "# RNN과 어텐션을 사용한 자연어 처리\n",
        "- 자연어 문제를 위해 많이 사용하는 방법은 순환 신경망임\n",
        "- 문자 단위 RNN: 문장에서 다음 글자를 예측\n",
        "  - 상태가 없는 RNN: 각 반복에서 무작위하게 택한 텍스트의 일부분으로 학습하고 나머지 텍스트에서 어떤 정보도 사용하지 않음 \n",
        "  - 상태가 있는 RNN: 훈련 반복 사이에 은닉 상태를 유지하고 중지된 곳에서 이어서 상태를 반영 -> 긴 패턴 학습 가능\n",
        "\n",
        "- 단어 단위(시퀀스) RNN\n",
        "- RNN 기반의 인코더-디코더 구조\n",
        "- 어텐션 메커니즘\n",
        "  - 각 타임 스텝에서 모델이 집중해야 할 입력 부분을 선택하도록 학습하는 신경망 구성 요소\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t1tcxWbP6o3"
      },
      "source": [
        "## Char-RNN을 사용해 셰익스피어 문체 생성하기\n",
        "- Char-RNN을 사용해서 한 번에 한 글자씩 새로운 텍스트를 생성할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyl5kR66QUue"
      },
      "source": [
        "### 훈련 데이터셋 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "386UaHDJQTtI",
        "outputId": "0fbee0b9-39ee-4c58-dbfb-8c247ab8cb76"
      },
      "source": [
        "url=\"https://homl.info/shakespeare\"\n",
        "filepath=keras.utils.get_file(\"shakespeare.txt\", url)\n",
        "with open(filepath) as f:\n",
        "  text=f.read()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://homl.info/shakespeare\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sMey42j1ulk",
        "outputId": "c1f5c9be-715c-4ddb-b8a3-ff1bbf7ee574"
      },
      "source": [
        "len(text)#=== 글자 수가 백만 개 이상"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NDyY_dPm1_ms",
        "outputId": "47163329-9c93-47e0-b8e8-a890ae16a4bd"
      },
      "source": [
        "text[:100]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OqYV93bS3D2"
      },
      "source": [
        "#### 모든 글자를 정수로 인코딩하기 -> 케라스의 Tokenizer클래스 아용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbkgptWCIGUy"
      },
      "source": [
        "tokenizer=keras.preprocessing.text.Tokenizer(char_level=True)#=== 단어 수준이 아닌 글자 수준 인코딩\n",
        "tokenizer.fit_on_texts(text)#=== 텍스트에 훈련"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMXgYSRDUBui"
      },
      "source": [
        "------\n",
        "텍스트에 사용되는 모든 글자를 찾아 글자 ID에 매핑(ID는 1부터 시작)\n",
        "<br><br>\n",
        "- 간단한 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ei-nN59cSyFZ",
        "outputId": "30dc044c-9690-460c-c879-8303e637a1f6"
      },
      "source": [
        "tokenizer.texts_to_sequences([\"First\"])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[20, 6, 9, 8, 3]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVUh-iBkTcte",
        "outputId": "0fdb2d0f-ed70-4d9e-8897-94eaef66e7c1"
      },
      "source": [
        "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['f i r s t']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v9-Y5NZT0Kd"
      },
      "source": [
        "- 전체 텍스트를 인코딩하여 각 글자를 ID로 나타내기\n",
        "  - 1부터 시작하므로 -1해서 0부터 시작하는 걸로 바꿔보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWI2YO-50Mma",
        "outputId": "4a8459f9-15cc-4509-aac0-df2c6122e068"
      },
      "source": [
        "max_id =len(tokenizer.word_index)\n",
        "max_id  #===== 즉, 고유 글자가 39개"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpDcZAPaToNt"
      },
      "source": [
        "[encoded] = np.array(tokenizer.texts_to_sequences([text]))-1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzX6PlQ7Tx5m",
        "outputId": "ebe3bd1a-3d15-4cd0-8147-371e4b18bcd6"
      },
      "source": [
        "encoded"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([19,  5,  8, ..., 20, 26, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygtDSNSoUjQR"
      },
      "source": [
        "### 순차 데이터셋을 나누는 방법\n",
        "- 훈련/검증/테스트 세트가 <strong>중복되지 않도록</strong> 만드는 것이 매우 중요\n",
        "  - 예를 들어 텍스트 처음 90%/5%/5%\n",
        "\n",
        "- 시계열을 다룰 때는 보통 <strong>시간</strong>에 따라 나눈다.(안전하다)\n",
        "  - ex)100개의 회사의 재정 건전성 관련 데이터 다루기 \n",
        "    - 많은 회사들이 <strong>강하게 상호 연관</strong>되어 있을 가능성이 높다.\n",
        "    - 훈련 세트와 테스트 세트에 상호 연관된 회사가 있다면 <strong>테스트 세트에서 측정한 일반 오차가 유용하지 않을 것임</strong>\n",
        "    - => 따라서 회사를 기준으로 나누는 등 다른 방법 보다 시간에 따라 나누는 것이 일반적이고, 안전함\n",
        "\n",
        "- 시계열이 <strong>충분히 안정적</strong>인지 꼭 확인해야한다.\n",
        "  - 일반적으로는, 시계열의 패턴이 '변하지 않는다'고 가정하지만, 일부는 그렇지 않음\n",
        "    - ex) 금융 시장은 변덕스럽다. 트레이더가 패턴을 발견한 뒤 적용하려 하면 사라짐\n",
        "  - 안정성 확인 방법: <strong>시간에 따라 검증 세트에 대한 모델의 오차 그려보기</strong>\n",
        "    - 모델이 검증 세트 <strong>마지막보다 첫 부분에서 성능이 더 좋다면</strong> 이 시계열이 충분히 안정되지 않은 것일 수 있음\n",
        "      - 이 경우, <strong>더 짧은 시간 간격</strong>으로 모델을 훈련하는 것이 좋다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvHO816PXzmk"
      },
      "source": [
        "#### train: 텍스트의 처음 90%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XFh_FrVUoBe",
        "outputId": "b5854665-035c-4d60-cfb4-cc6ea23c2dda"
      },
      "source": [
        "dataset_size=tokenizer.document_count#== 전체 글자 개수\n",
        "train_size=dataset_size*90//100 #=== 텍스트의 처음 90%\n",
        "dataset= tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
        "dataset"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TensorSliceDataset shapes: (), types: tf.int64>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oogztBsCY7XI"
      },
      "source": [
        "### 순차 데이터를 윈도 여러 개로 자르기\n",
        "- 해당 text는 백만 개 이상의 글자로 이루어진 시퀀스 하나\n",
        "- 즉, 이대로 RNN으로 훈련 시키면, 백만 개의 층이 있는 심층 신경망과 같게 된다.\n",
        "- 따라서 <strong>window()메서드</strong>를 사용해서 긴 문자열을 짧은 부분 문자열로 변환한다.\n",
        "- 이 짧은 부분 문자열만큼만 역전파하는게 <strong>TBPTT(Truncated Backpropagation Through Time)</strong>\n",
        "- BPTT vs TBPTT\n",
        "\n",
        "  <img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdawXWw%2FbtqD2dngo5l%2FrvitZfPrq5bXlHPUfy07J0%2Fimg.png\" width=50% />\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbiRY38%2FbtqD2bweMfV%2FXiaxKlF44miJP4IHATTfQk%2Fimg.png\" width=50%/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kemV5qKyk0j"
      },
      "source": [
        "n_steps=100\n",
        "window_length=n_steps+1\n",
        "dataset=dataset.window(window_length,shift=1,drop_remainder=True) "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRVMTGlT5pyc"
      },
      "source": [
        "--------------\n",
        "- window_length: 한 번에 얼마나 볼것인지(너무 짧게 만들면 긴 패턴을 학습할 수 없으므로 적절하게 조절해야함)\n",
        "- shift: 윈도우를 얼마만큼 움직일 것인지(기본값=window 길이 )\n",
        "- drop_remainder:  True로 하면 모든 윈도우가 동일한 개수의 글자를 포함하도록한다.\n",
        "<br><br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-x8nWnf5Y-b"
      },
      "source": [
        "dataset= dataset.flat_map(lambda window: window.batch(window_length)) #=== 윈도우마다 batch(window_length)호출"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNI1-yMA9z0W"
      },
      "source": [
        "--------\n",
        "- flat_map()메서드를 통해서 데이터셋을 변환하고 평평하게 만든다\n",
        "- ex) 기존 데이터셋(ds): {{1,2},{3,4,5,6}} <br>\n",
        "flat_map(lambda ds: ds.batch(2)) ->   {[1,2],[3,4],[5,6]} : 텐서 2개를 가진 데이터셋\n",
        "<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUdUvYkqAyeG"
      },
      "source": [
        "- 섞고, 타깃 값(마지막 1개 글자) 분리\n",
        "\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FY65A7%2FbtqEWB10RrH%2Ff1Ngbw1V4OTxuhbpKRlkr1%2Fimg.png\"/>\n",
        "\n",
        "- 위 예시는 윈도 크기가 11 , 배치 크기가 3인 경우이다.\n",
        "- input이 [2,12] 구간이라면, target은 [3,13]구간의 문자가 되겠다.\n",
        "  - 즉, [2,12]구간의 문자들로 학습을 하면 13번째 문자를 추가로 맞춰야 하는 것"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsFd4L4v-6KV"
      },
      "source": [
        "batch_size=32\n",
        "dataset = dataset.shuffle(10000).batch(batch_size)\n",
        "dataset=dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFUhi4LZBcZV"
      },
      "source": [
        "- 입력 특성 원-핫 벡터\n",
        "  - 고유한 글자 수가 적기 때문(39개)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHq_JWIuAXLt"
      },
      "source": [
        "dataset= dataset.map(\n",
        "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch)\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Qyh8peiAoWP"
      },
      "source": [
        "dataset = dataset.prefetch(1)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a79R_tdEB6Ok"
      },
      "source": [
        "### Char-RNN 모델 만들고 훈련하기\n",
        "- 이전 글자 100개를 기반으로 다음 글자를 예측하기 위해 \n",
        "  - 유닛 128개를 가진 GRU층 2개\n",
        "  - 입력과 은닉 상태에 20% 드롭아웃(dropout, recurrent_dropout)\n",
        "  - 출력층은 각 글자에 대한 확률을 출력할 것이므로 softmax, 유닛 max_id개"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4hhFHGsB4uf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d38fd4b-8162-4078-d7fd-9e009647bdf4"
      },
      "source": [
        "model= keras.models.Sequential([\n",
        "                                keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id], dropout=0.2, recurrent_dropout=0.2),\n",
        "                                keras.layers.GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
        "                                keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation='softmax'))\n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru (GRU)                    (None, None, 128)         64896     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, None, 128)         99072     \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, None, 39)          5031      \n",
            "=================================================================\n",
            "Total params: 168,999\n",
            "Trainable params: 168,999\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDjbf2MKlIsg",
        "outputId": "546310aa-cdba-4d04-9e6e-3500f4982ed0"
      },
      "source": [
        "cd train"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/HandsOn/train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZr0uadekyQw"
      },
      "source": [
        "#==학습이 오래 걸리니 중간 중간 저장하자\n",
        "#check_point = keras.callbacks.ModelCheckpoint('rnn_{epoch:02d}.h5', monitor='loss', save_freq=3)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam')\n",
        "history=model.fit(dataset, epochs=20, callbacks=[check_point]) #====너어어어ㅓ무 오래 걸려서 나중에..."
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btCNlj58ib9U"
      },
      "source": [
        "def preprocess(texts):\n",
        "  X=np.array(tokenizer.texts_to_sequences(texts)) -1\n",
        "  return tf.one_hot(X,max_id)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F2GbM4epCVq"
      },
      "source": [
        "X_new=preprocess([[\"How are yo\"], [\"I'm fin\"]])\n",
        "y_pred=model.predict_classes(X_new)\n",
        "tokenizer.sequences_to_texts(y_pred+1)[0][-1] #첫 번째 문장을 봤을 때 이어질 다음 문자는? -> 학습 20 에포크동안 하면 'u' 잘 나온다고 함"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmAeuKRQpFLw"
      },
      "source": [
        "### 가짜 셰익스피어 텍스트 생성하기\n",
        "Char-RNN모델로 새로운 텍스트를 생성하려면?\n",
        "- Char-RNN 모델이 예측한 다음 글자를 텍스트 끝에 추가 -> input -> 추가 -> input .... \n",
        "  - 근데 이 방식으로 하면 같은 단어가 계속 반복 됨\n",
        "<br>\n",
        "- tf.random.categorical()함수를 사용해서 모델이 추정한 확률을 기반으로 다음 글자를 무작위로 선택한다면 더 다채로운 텍스트 생성 가능\n",
        "  - 다양성 제어 파라미터: temperature\n",
        "    - 예측 확률에 로그를 취하고(로짓) temperature로 나눔\n",
        "    - 0에 가까울수록 높은 확률을 가진 글자가 선택될 가능성이 더 높아져서 다채로움이 사라짐"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAbsy3LTo7__"
      },
      "source": [
        "def next_char(text, temperature=1):\n",
        "  X_new = preprocess([text])\n",
        "  y_proba= model.predict(X_new)[0,-1:,:]\n",
        "  rescaled_logits=tf.math.log(y_proba)/temperature #==== temperature로 나누는 부분\n",
        "  char_id=tf.random.categorical(rescaled_logits, num_samples=1)+1\n",
        "  return tokenizer.sequences_to_texts(char_id.numpy())[0]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7Hv2DxlqmRt"
      },
      "source": [
        "- next_char()함수를 반복 호출해서 다음 글자를 얻고, 텍스트에 추가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-55_ZjDeqCW1"
      },
      "source": [
        "def complete_text(text, n_chars=50, temperature=1):\n",
        "  for _ in range(n_chars):\n",
        "    text += next_char(text, temperature)\n",
        "  return text"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXu3ipavtWpQ"
      },
      "source": [
        "----------\n",
        "- 해당 모델은 1에 가까운 온도에서 가장 잘 작동한다고 함\n",
        "- 해당 모델은 n_steps=100을 넘어가면 훈련이 더 어려워질 것임\n",
        "  - LSTM, GRU셀도 매우 긴 시퀀스는 다루기 어렵다<br>\n",
        "=> 상태가 있는 RNN을 살펴보자"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2y-srzZty-3"
      },
      "source": [
        "### 상태가 있는 RNN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69Lre_ErtUpU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}